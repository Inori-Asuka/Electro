defaults:
  - _self_

exp: dinov3_convnext_large

data:
  data_root: /data/wly/data/eletric
  image_size: 224

# DINOv3 配置 
dinov3:
  # 模型类型: 'convnext', 'dinov3-vit', 'vit', 'resnet', 'swin-t'
  # 'convnext': DINOv3 ConvNeXt-Large (需要 model_path)
  # 'dinov3-vit': DINOv3 ViT Large (需要 model_path)
  # 'vit': 普通 ViT (使用 timm 预训练，需要 model_name 和 pretrained)
  # 'resnet': ResNet (使用 timm 预训练，需要 model_name 和 pretrained)
  # 'swin-t': Swin Transformer (使用 timm 预训练，需要 model_name 和 pretrained)
  backbone_type: convnext
  
  model_path: /data/wly/ckpt/dinov3_convnext_large/model.safetensors
  
  pooling: gap  # 'linear', 'gap', 'attention', 'multiscale'
  
  multiscale_layers: null  # null: ConvNeXt=[1,2,3], ViT=[6,12,18,23]
  
  head_type: linear # 'linear', 'mlp', 'attention', 'gated', 'residual'

  # freeze_backbone: true
  freeze_backbone: false  # 全参数微调
  # freeze_layers: ['blocks.0', 'blocks.1']  # 部分冻结

  use_lora: true  # LoRA微调
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1

  lora_modules: null  # null 或 ['qkv'], ['mlp'], ['qkv', 'mlp'], ['qkv', 'proj', 'mlp'] 等
  
  head_kwargs:  # 回归头的额外参数
    hidden_dims: [512, 256, 128]

    # pooling='attention' :
    # num_heads: 8
    # hidden_dim: null

    # pooling='multiscale' :
    # hidden_dims: [2048, 1024] 

training:
  batch_size: 8
  epochs: 500
  lr: 1.0e-03 
  weight_decay: 1.0e-05
  optimizer: adam  # 'adam', 'adamw', 'sgd'
  patience: 75
  best_metric: entry_mae_denorm  # 'entry_mae_denorm', 'entry_rmse', 'val_mae_denorm'
  dropout: 0.5

loss:
  type: smooth_l1  # 'mse', 'smooth_l1', 'huber'
  smooth_l1_beta: 1.0
  huber_delta: 1.0

scheduler:
  type: cosine  # 'plateau', 'cosine', 'onecycle', null
  warmup_epochs: 0

normalization:
  method: null # 'standard', 'minmax', 'robust', null

misc:
  num_workers: 4
  seed: 42
  max_grad_norm: 1.0
  use_tensorboard: true
  log_dir: /data/wly/ckpt/Tem_eapp/logs/${exp}
  checkpoint_dir: /data/wly/ckpt/Tem_eapp/checkpoints/${exp}